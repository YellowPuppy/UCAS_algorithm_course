\documentclass{beamer}
%\usepackage{beamerthemeshadow}
\usepackage{beamerthemesplit}
%\usetheme{shadow}
\usepackage{graphicx}
\usecolortheme{lily}

\title{Computing Bi-Clusters for Microarray Analysis}
\author{Yu Lin}
\date{\today}

\begin{document}

\frame{\titlepage}

%\section[Outline]{}
%\frame{\tableofcontents}

\section{Bi-clustering Problem}

    \subsection{General Bi-clustering Problem }
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
%    \center{Definition of Bi-clustering Problem}\\
    \item Input: a $n \times m$ matrix $A$.
    \item Output: a sub-matrix $A_{P,Q}$ of $A$ such that the rows of $A_{P,Q}$  are {\it similar}.
    That is, all the rows are identical.

    Why sub-matrix?\\
    A subset of  {\it genes} are co-regulated and co-expressed under specific {\it conditions}.
    It is interesting to find the subsets of genes and conditions.
    \end{itemize}
    }

    \subsection{Similarity of Rows (1-5)}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item 1.  All rows are identical \\
       $1 ~1 ~2 ~3 ~2 ~3 ~3 ~2$ \\
       $1 ~1 ~2 ~3 ~2 ~3 ~3 ~2$ \\
       $1 ~1 ~2 ~3 ~2 ~3 ~3 ~2$ \\
    \item 2.  All the elements in a row are identical\\
       $1 ~1 ~1 ~1 ~1 ~1 ~1 ~1$\\
       $2 ~2 ~2 ~2 ~2 ~2 ~2 ~2$\\
       $5 ~5 ~5 ~5 ~5 ~5 ~5 ~5$\\
       (the same as 1 if we treat columns as
       rows)
    \end{itemize}
    }

    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item 3.  The curves for all rows are similar (additive)
       $a_{i,j}-a_{i,k} = c(j,k)$  for $i=1, 2, \ldots, m.$
       Case 3 is equivalent to case 2 (thus also case 1)  if we construct
       a new matrix $a^{*}_{i,j} =a_{i,j}-a_{i,p}$ for a fixed $p$ indicate a row.
       \begin{figure}
        \centering
        \includegraphics[scale=0.3]{curve.jpg}
        \end{figure}
    \end{itemize}
    }

    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item 4. The curves for all rows are similar
    (multiplicative)\\
    $~~a_{1,1} ~~~~~ a_{1,2}~~~~~ a_{1,3}~~~~\ldots~~~~    a_{1,m}$ \\
    $c_{1}a_{1,1} ~~ c_{1}a_{1,2}~~ c_{1}a_{1,3} ~~~\ldots~~~
    c_{1}a_{1,m}$\\
    $c_{2}a_{1,1} ~~  c_{2}a_{1,2}~~  c_{2}a_{1,3} ~~~\ldots ~~~
    c_{2}a_{1,m}$\\
    $~~~\ldots ~~~$\\
    $c_{n}a_{1,1} ~~  c_{n}a_{1,2}~~  c_{n}a_{1,3} ~~~\ldots ~~~
    c_{n}a_{1,m}$
    \newline

    Transfer to case 2 (thus case 1) by taking log and
    subtraction.\\
    Case 3 and Case 4 are called bi-clusters with coherent values.
    \end{itemize}
    }

    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item 5.  The curves for all rows are similar
    (multiplicative and additive)
    \newline

    $a_{i,j}=c_{i}¡Áa_{k,j} +d_{i}$
    \newline

    Transfer to case 2 (thus case 1) by subtraction of a fixed row
    (row i), taking log and subtraction of row i again.
    \newline
    The basic model: All the rows in the sub-matrix are identical.
    \end{itemize}
    }

    \subsection{Cheng and Church¡¯s model}
    \frame {
    \frametitle{\subsecname}
    The model introduced a similarity score called the mean squared residue score H  to measure the coherence of the rows and columns in the submatrix.
    \begin{eqnarray}
    H(P,Q)=\frac{1}{|P||Q|}\sum_{i\in P, j\in Q}(a_{i,j}-a_{i,Q}-a_{P,j}+a_{P,Q})^{2} \nonumber
    \end{eqnarray}
    where
    \begin{eqnarray}
    a_{i,Q}= \frac{1}{|Q|}\sum_{j\in Q}a_{i,j},~~ a_{P,j}=\frac{1}{|P|}\sum_{i\in P}a_{i,j}, a_{P,Q}=\frac{1}{|P||Q|} \sum_{i\in P, j\in Q}a_{i,j}. \nonumber
    \end{eqnarray}
    If there is no error, H(P, Q)=0 for case 1, 2 and 3. A lot of heuristics (programs) have been produced.
    }


\section{Our Problem Definition}

   \frame {
    \frametitle{\secname}
    \begin{itemize}
    \item Consensus Sub-matrix Problem \\
    \item Bottleneck  Sub-matrix Problem
    \end{itemize}
    }

    \subsection{Consensus Sub-matrix Problem}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item Input: a $n \times m$ matrix $A$, integers $l$ and $k$.
    \item Output: a sub-matrix $A_{P,Q}$ of $A$ with $l$ rows and $k$ columns and a consensus row $z$ (of $k$ elements)  such
    that
    \newline

    $\sum_{r_{i}\in P}d(r_{i}|^{Q},z)$ is minimized.
    \newline

    Here $d(~,~)$ is the Hamming distance.
    \end{itemize}
    }

    \subsection{Bottleneck  Sub-matrix Problem}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item Input: a $n \times m$ matrix $A$, integers $l$ and $k$.
    \item Output: a sub-matrix $A_{P,Q}$ of $A$ with $l$ rows and $k$ columns and a consensus row $z$ (of $k$ elements) such
    that for any $r_{i}$ in $P$
    \newline

    $d(r_{i}|^{Q},z) \leq d$ and $d$ is minimized
    \newline

    Here $d(~,~)$ is the Hamming distance.
    \end{itemize}
    }

    \subsection{NP-Hardness Results}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item Theorem 1: Both consensus sub-matrix and bottleneck sub-matrix problems are NP-hard.
    \newline

    Proof: We use a reduction from maximum edge bipartite problem.
    \end{itemize}
    }


\section{Approximation Algorithm for Consensus Sub-matrix Problem}

   \frame {
    \frametitle{\secname}
    \begin{itemize}
    \item Input: a $n \times m$ matrix $A$, integers $l$ and $k$.
    \item Output: a sub-matrix $A_{P,Q}$ of $A$ with $l$ rows and $k$ columns and a consensus row $z$ (of $k$ elements)  such
    that
    \newline

    $\sum_{r_{i}\in P}d(r_{i}|^{Q},z)$ is minimized.
    \newline

    Here $d(~,~)$ is the Hamming distance.
    \end{itemize}
    }

    \subsection{Basic Ideas}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \footnotesize{
    \item Assumptions: $H_{opt}=\sum_{p_{i}\in
    P_{opt}}d(x_{p_{i}}|^{Q_{opt}},z_{opt})=O(kl)$, $H_{opt}
    \times c' = kl$ and $|Q_{opt}|=k=O(n)$, $k \times c =n$.
    \item Basic Ideas:  We use a random sampling technique to randomly select O(log$m$) columns in $Q_{opt}$,
    enumerate all possible vectors of length O(log$m$) for those columns. At some moment, we know O(log$m$)
    bits of $r_{opt}$ and we can use the partial $z_{opt}$ to select the $l$ rows which are closest to $z_{opt}$ in those O(log$m$) bits.
    After that we can construct a consensus vector $r$ as follows: for each column, choose the (majority) letter that appears the most in each of the
    $l$ letters in the $l$ selected rows. Then for each of the $n$ columns, we can calculate the number of mismatches between the majority letter
    and the $l$ letters in the $l$ selected rows. By selecting the best
    $k$ columns, we can get a good solution.
    }
    \end{itemize}
    }

    \frame {
    \frametitle{\subsecname}
        \begin{itemize}
        \item  How to randomly select O(log$m$) columns in $Q_{opt}$ while $Q_{opt}$ is unknown?
        \item Our new idea is to randomly select a set B of  $(c+1)$log$m$ columns from $A$ and enumerate all size log$m$ subsets of $B$ in time
        $O(m^{c+1})$ which is polynomial in terms of the input size $O(mn)$. We can show that with high probability, we can get a set of log$m$ columns
        randomly selected from $Q_{opt}$.
        \end{itemize}
    }

    \subsection{Approximation Algorithm}

    \frame {
    \textbf{ Algorithm 1 for The Consensus Submatrix Problem }\\
    \footnotesize{   \textbf{Input:}   one $m\times n$ matrix $A$, integers $l$ and $k$, and $\epsilon>
    0$\\
    \textbf{Output:}  a size $l$ subset $P$ of rows, a size $k$ subset $Q$ of columns and a length $k$ consensus vector
    $z$\\
    \textbf{Step 1:} randomly select a set $B$ of $\lceil(c+1)(\frac{4\log m}{\epsilon^{2}}+1)\rceil$ columns from
    $A$.\\
    (1.1) {\bf for}  every size $\lceil\frac{4\log m}{\epsilon^{2}}\rceil$  subset $R$ of $B$ {\bf
    do}\\
    (1.2) {\bf for}  every $z|^{R} \in \Sigma^{|R|}$ {\bf
    do}\\
    (a) {Select the best $l$ rows $P =\{p_{1},...,p_{l}\}$ that minimize $d(z|^{R}, x_{i}|^{R})$.}\\
    (b)  {{\bf for}  each column $j$ {\bf do}\\
    Compute $f(j)=\sum_{i=1}^{l}d(s_{j},a_{p_{i},j})$, where $s_{j}$ is the  majority element of the $l$ rows in $P$ in column
    $j$. Select the best $k$ columns $Q=\{q_{1},...,q_{k}\}$ with minimum value $f(j)$ and let $z(Q)=s_{q_{1}}s_{q_{2}}\ldots
    s_{q_{k}}$.}\\
    (c) {Calculate $H=\sum_{i=1}^{l}d(x_{p_{i}}|^{Q},z)$  of this
    solution.}\\
    \textbf{Step 2:} Output $P$, $Q$ and $z$ with minimum $H$. }
    }

    \subsection{Proofs}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
        \item  Lemma 1: With probability at most $m^{-\frac{2}{\epsilon^{2}c^{2}(c+1)}}$, no subset $R$ of size $\lceil\frac{4\log m}{\epsilon^{2}}\rceil$ used in Step 1 of Algorithm 1 satisfies  $R \subseteq
        Q_{opt}$.\\
        \item  Lemma 2: Assume $|R|=\lceil\frac{4\log m}{\epsilon^{2}}\rceil$ and $R \subseteq Q_{opt}$. Let $\rho = \frac{k}{|R|}$.
        With probability at most  $m^{-1}$, there is a row $x_{i}$ in $X$ satisfying
        \begin{eqnarray}
        \frac{d(z_{opt}, x_{i}|^{Q_{opt}})-\epsilon k}{\rho}>d(z_{opt}|^{R}, x_{i}|^{R})
        .\nonumber
        \end{eqnarray}
        With probability at most  $m^{-\frac{1}{3}}$, there is a row $x_{i}$ in $X$ satisfying
        \begin{eqnarray}
        d(z_{opt}|^{R}, x_{i}|^{R}) > \frac{d(z_{opt}, x_{i}|^{Q_{opt}})+\epsilon k}{\rho}
        .\nonumber
        \end{eqnarray}
        \end{itemize}
    }

    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item Lemma 3: When $R \subseteq Q_{opt}$ and $z|^{R}=z_{opt}|^{R}$, with probability at most $2 m^{-\frac{1}{3}}$,
    the set of rows $P=\{p_{1},\ldots,p_{l}\}$ selected in Step 1 (a) of Algorithm 1 satisfies $\sum_{i=1}^{l}d(z_{opt}, x_{p_{i}}|^{Q_{opt}}) > H_{opt} + 2\epsilon kl $.
    \item Theorem 2: For any $\delta >0$, with probability at least $1-m^{-\frac{8c'^{2}}{\delta^{2}c^{2}(c+1)}}-2 m^{-\frac{1}{3}}$,
    Algorithm 1 will output a solution with consensus score at most $(1+ \delta)H_{opt}$ in  $O(nm^{O(\frac{1}{\delta^{2}})})$ time.
    \end{itemize}
    }


\section{Approximation Algorithm for Bottleneck Sub-matrix Problem}

   \frame {
    \frametitle{\secname}
    \begin{itemize}
    \item Input: a $n \times m$ matrix $A$, integers $l$ and $k$.
    \item Output: a sub-matrix $A_{P,Q}$ of $A$ with $l$ rows and $k$ columns and a consensus row $z$ (of $k$ elements) such
    that for any $r_{i}$ in $P$
    \newline

    $d(r_{i}|^{Q},z) \leq d$ and $d$ is minimized
    \newline

    Here $d(~,~)$ is the Hamming distance.
    \end{itemize}
    }

    \subsection{Basic Ideas}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item Assumptions: $d_{opt}=MAX_{p_{i}\in P_{opt}}d(x_{p_{i}}|^{Q_{opt}},z_{opt})=O(k)$, $d_{opt} \times c''= k$ and $|Q_{opt}|=k=O(n)$, $k \times c =n$.
    \item Basic Ideas:\\
    (1) Use random sampling technique to know O(log$m$) bits of $z_{opt}$ and select $l$ best rows like Algorithm 1.\\
    (2) Use linear programming and randomized rounding technique to select $k$ columns in the matrix.
    \end{itemize}
    }

   \frame {
    \begin{itemize}
    \item Linear programming\\
   \footnotesize{ Given a set of rows $P= \{ p_{1},\ldots,p_{l} \}$, we want to find a set of $k$ columns $Q$ and vector $z$ such that bottleneck score is
    minimized.\\
    \begin{eqnarray}
    && \min d; \nonumber \\
    && \sum_{i =1}^{n} \sum_{j=1}^{|\Sigma|}y_{i,j} = k, \nonumber \\
    && \sum_{j=1}^{|\Sigma|}y_{i,j} \leq 1, i = 1,2,\ldots,n, \nonumber \\
    && \sum_{i=1}^{n}\sum_{j=1}^{|\Sigma|}\chi(\pi_{j},x_{p_{s},i})y_{i,j}\leq d, s = 1,2,\ldots,l.\nonumber
    \end{eqnarray}
    }
    $y_{i,j}=1$ if and only if column $i$ is in $Q$ and the corresponding bit in $z$ is $\pi_{j}$.\\
    Here, for any $a,b \in \Sigma$, $\chi(a,b) = 0$ if $a = b$ and $\chi(a,b)=1$ if $a \neq b$.
    \end{itemize}
        }

   \frame {
    \begin{itemize}
    \item Randomized rounding\\
    To achieve two goals:\\
    (1) Select $k'$ columns, where $k'\geq k-\delta d_{opt}$.\\
    (2) Get integers values for $y_{i,j}$ such that the distance (restricted on the $k'$ selected columns) between any row in
    $P$
    and the center vector thus obtained is at most
    $(1+\gamma)d_{opt}$.\\
    Here $\delta>0$ and $\gamma>0$ are two parameters used to control the errors.
    \end{itemize}
        }

    \frame {
    \begin{itemize}
    \item Lemma 4: When $\frac{n\gamma^{2}}{3(cc'')^{2}} \geq 2\log m$, for any $\gamma, \delta>0$, with probability at most
    $exp(-\frac{n\delta^{2}}{2(cc'')^{2}}) + m^{-1}$, the rounding result $y'=\{y'_{1,1},\ldots,y'_{1,|\Sigma|}, \ldots, y'_{n,1},\ldots,y'_{n,|\Sigma|}\}$ does not satisfy
    at least one of the following inequalities,
    \begin{eqnarray}
    \sum_{i=1}^{n}(\sum_{j=1}^{|\Sigma|}y'_{i,j}) > k - \delta
    d_{opt},\nonumber
    \end{eqnarray}
    and for every row $x_{p_{s}}(s = 1,2,\ldots,l)$,
    \begin{eqnarray}
    \sum_{i =1}^{n}
    (\sum_{j=1}^{|\Sigma|}\chi(\pi_{j},x_{p_{s},i})y'_{i,j}) <
    \overline{d} + \gamma d_{opt}.\nonumber
    \end{eqnarray}
    \end{itemize}
        }


    \subsection{Approximation Algorithm}
    \frame {
    \textbf{Algorithm 2 for The bottleneck Sub-matrix Problem}
    \footnotesize{
    \textbf{Input:}   one matrix $A \in \Sigma^{m \times n}$, integer $l$, $k$, a row $z \in \Sigma^{n}$ and  small numbers $\epsilon > 0$, $\gamma>0$ and $\delta
    >0$.\\
    \textbf{Output:}  a size $l$ subset $P$ of rows, a size $k$ subset $Q$ of columns and a length $k$ consensus vector
    $z$.\\
    \textbf{if} $\frac{n\gamma^{2}}{3(cc'')^{2}} \leq  2\log m$ {\bf then} {try  all size $k$ subset $Q$ of the $n$ columns and all $z$ of length $k$ to solve the
    problem.}\\
    \textbf{if} $\frac{n\gamma^{2}}{3(cc'')^{2}} > 2\log m$ {\bf
    then}\\
    \textbf{Step 1:} randomly select a set $B$ of $\lceil\frac{4(c+1)\log m}{\epsilon^{2}}\rceil$ columns from $A$.
    {\bf for} every $\lceil\frac{4\log m}{\epsilon^{2}}\rceil$ size subset $R$ of $B$ {\bf
    do}\\
    {\bf for} every $z|^{R} \in \Sigma^{|R|}$ {\bf do}\\
    (a) Select the best $l$ rows $P =\{p_{1},...,p_{l}\}$ that minimize $d(z|^{R},
    x_{i}|^{R})$.\\
    (b)Solve the optimization problem by linear programming and randomized rounding to get $Q$ and
    $z$.\\
    \textbf{Step 2:} Output $P$,$Q$ and $z$ with minimum bottleneck score $d$.
    }
    }

    \subsection{Proofs}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item Lemma 5: When $R\subseteq Q_{opt}$ and $z|^{R}=z_{opt}|^{R}$, with probability at most  $2m^{-\frac{1}{3}}$, the  set of rows
    $P=\{p_{1},\ldots,p_{l}\}$ obtained in Step 1(a) of  Algorithm 2 satisfies $d(z_{opt}, x_{p_{i}}|^{Q_{opt}})> d_{opt} + 2\epsilon k $ for some row $x_{p_{i}}(1\leq i \leq l)$.
    \item Theorem 3: With probability at least $1-m^{-\frac{2}{\epsilon^{2}c^{2}(c+1)}}-2m^{-\frac{1}{3}}- exp(-\frac{n\delta^{2}}{2(cc'')^{2}}) -m^{-1}$, Algorithm 2 runs in time
    $O(n^{O(1)}m^{O(\frac{1}{\epsilon^{2}}+\frac{1}{\gamma^{2}})})$ and obtains a solution with bottleneck score at most $(1 + 2c''\epsilon+\gamma +\delta)d_{opt}$ for any fixed $\epsilon,~\gamma, ~\delta> 0$.
    \end{itemize}
    }

   \subsection{Thanks}
    \frame {
    \frametitle{\subsecname}
    \begin{itemize}
    \item Acknowledgements \\
    This work is fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China [Project No. CityU
    1070/02E].\\
    This work is collaborated with Dr. Lusheng Wang and Xiaowen
    Liu in City University of Hong Kong, Hong Kong, China.
    \end{itemize}
    }

    \frame{
    Let $X_1,X_2,\ldots,X_n$ be $n$ independent random 0-1 variables,
    where $X_i$ takes $1$ with probability $p_i$, $0<p_i<1$. Let
    $X=\sum _{i=1}^n X_i$, and $\mu=E[X]$. Then for any
    $0<\epsilon\leq 1$,
    \begin{eqnarray}
    {\bf Pr}(X>\mu+\epsilon\, n ) &<& e^{-\frac 13 n \epsilon ^2} ,\nonumber \\
    {\bf Pr}(X<\mu-\epsilon\,n ) &\leq& e^{ -\frac 12 n \epsilon ^2}.
    \nonumber
    \end{eqnarray}
    }

\end{document}
